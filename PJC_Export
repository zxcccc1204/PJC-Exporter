# coding = utf-8

"""åŒ¯å…¥æ‰€éœ€åˆ°å‡½å¼åº«"""
import datetime
import json
import logging
import os
import shutil
import tkinter as tk
from datetime import timedelta, timezone
from tkinter import messagebox, scrolledtext
from calendar_widget import Calendar
import gspread
import numpy as np
import pandas as pd
import pytz
import requests
from cryptography.fernet import Fernet
from oauth2client.service_account import ServiceAccountCredentials
from pandas import json_normalize
from tkcalendar import Calendar
from datetime import datetime
from tkinter import font as tkfont

"""å…¨åŸŸé…ç½®æŒ‡å®šè·¯å¾‘"""
config_file_path = 'config.json'
temp_directory = 'temp'

if os.path.exists(temp_directory):
    shutil.rmtree(temp_directory)

"""å®šç¾©æ—¥è¨˜æª”"""
def setup_logging(temp_directory, log_file_path, error_log_path):
    # æ¯ä¸€æ¬¡éƒ½æœƒæ–°å¢ä¸€å€‹tempå­˜æ”¾log
    if not os.path.exists(temp_directory):
        os.makedirs(temp_directory)

    # é…ç½®å…¨å±€æ—¥èªŒ
    logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s', encoding='utf-8')

    # é…ç½®æª¢æŸ¥çš„æ—¥èªŒ
    error_logger = logging.getLogger('errorLogger')
    error_handler = logging.FileHandler(error_log_path, encoding='utf-8')
    error_handler.setLevel(logging.INFO)
    error_formatter = logging.Formatter('%(asctime)s %(levelname)s:%(message)s')
    error_handler.setFormatter(error_formatter)
    error_logger.addHandler(error_handler)

    return error_logger

"""é…ç½®æ–‡ä»¶æª”è¨­å®š"""
def load_config(config_file_path):
    try:
        with open(config_file_path, 'r') as file:
            return json.load(file)
    except Exception as e:
        logging.error(f"âŒ ç„¡æ³•åŠ è¼‰é…ç½®æ–‡ä»¶ {config_file_path}: {e}")
        return None

"""UIä»‹é¢è¨­å®š(ç”¨ä¾†æ›´æ”¹configæª”æ—¥æœŸ)"""
def create_date_selector(root, config_file_path):
    date_selector_window = tk.Toplevel(root)
    date_selector_window.title('é¸æ“‡æ—¥æœŸ')
    date_selector_window.geometry('300x700')
    date_selector_window.resizable(True, True)
    date_selector_window.configure(bg='lightblue')  # è¨­å®šåº•è‰²

    # èµ·å§‹æ—¥æœŸé¸æ“‡
    tk.Label(date_selector_window, text='é–‹å§‹æ—¥æœŸ', font=('Microsoft JhengHei', 12, 'bold'), bg='lightblue').pack(pady=(10, 0))
    start_calendar = Calendar(
        date_selector_window,
        selectmode='day',
        date_pattern='yyyy-mm-dd',
        background='lightblue',
        foreground='black',
        selectbackground='blue',
        selectforeground='red',
        disabledbackground='grey',
        disabledforeground='lightgrey',
        font=('Microsoft JhengHei', 12),
        showweeknumbers=False,
        firstweekday='sunday'
    )
    start_calendar.pack(pady=(0, 10))

    # çµæŸæ—¥æœŸé¸æ“‡
    tk.Label(date_selector_window, text='çµæŸæ—¥æœŸ', font=('Microsoft JhengHei', 12, 'bold'), bg='lightblue').pack(pady=(10, 0))
    end_calendar = Calendar(
        date_selector_window,
        selectmode='day',
        date_pattern='yyyy-mm-dd',
        background='lightblue',
        foreground='black',
        selectbackground='blue',
        selectforeground='red',
        disabledbackground='grey',
        disabledforeground='lightgrey',
        font=('Microsoft JhengHei', 12),
        showweeknumbers=False,
        firstweekday='sunday'
    )
    end_calendar.pack(pady=(0, 20))

    # å„²å­˜ä¸¦ç”¢ç”Ÿ CSV
    def save_dates():
        try:
            start_date = start_calendar.get_date()
            end_date = end_calendar.get_date()

            if end_date < start_date:
                messagebox.showwarning("æ—¥æœŸéŒ¯èª¤", "çµæŸæ—¥æœŸä¸å¯æ—©æ–¼é–‹å§‹æ—¥æœŸ")
                return

            # è®€å–é…ç½®æª”æ¡ˆ
            with open(config_file_path, 'r', encoding='utf-8') as file:
                config = json.load(file)

            # æ›´æ–°æ—¥æœŸå€é–“
            start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            end_datetime = datetime.strptime(end_date, "%Y-%m-%d")

            config["time_range"]["start_time"] = start_datetime.strftime("%Y-%m-%d 00:00:00")
            config["time_range"]["end_time"] = end_datetime.strftime("%Y-%m-%d 02:00:00")

            # å¯«å…¥å›é…ç½®æª”æ¡ˆ
            with open(config_file_path, 'w', encoding='utf-8') as file:
                json.dump(config, file, indent=4, ensure_ascii=False)

            messagebox.showinfo("æˆåŠŸ", "æ—¥æœŸå·²æˆåŠŸä¿å­˜ï¼Œè«‹ä¸è¦é—œé–‰ä¸»è¦–çª—!\né»é¸ç¢ºå®šä»¥ç¹¼çºŒï¼")

            date_selector_window.destroy()

            # å‘¼å«å¤–éƒ¨å‡½å¼
            export_data(root)

        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"ä¿å­˜æ—¥æœŸæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š\n{e}")

    save_button_style = {
        "font": ("Microsoft JhengHei", 12, "bold"),
        "bg": "Pink",           # ç¶ è‰²èƒŒæ™¯
        "fg": "Black",             # ç™½è‰²å­—é«”
        "activebackground": "Yellow",  # é»æ“Šæ™‚çš„èƒŒæ™¯è‰²
        "width": 21,
        "height": 2,
        "relief": "raised",        # æµ®å‡¸æ•ˆæœ
        "bd": 2                    # é‚Šæ¡†å¯¬åº¦
    }

    tk.Button(date_selector_window, text='ä¿å­˜æ—¥æœŸï¼Œä¸¦é–‹å§‹ç”¢ç”Ÿ CSV',
            command=save_dates, **save_button_style).pack(pady=10)

# æœˆå ±å€é–“é¸æ“‡
def monthly_report(root, config_file_path):
    entry_window = tk.Toplevel(root)
    entry_window.title("Report Month")
    entry_window.geometry('300x200')
    entry_window.resizable(False, False)
    entry_window.configure(bg='lightblue') 

    def get_date_range():
        try:
            month_input = entry.get()
            year = int(month_input[:4])
            month = int(month_input[4:])
            start_date = datetime(year, month, 1)
            end_date = start_date.replace(day=1, month=start_date.month % 12 + 1) - timedelta(days=1)

            with open(config_file_path, 'r') as file:
                config = json.load(file)

            # æ›´æ–°é…ç½®æ–‡ä»¶
            config["monthly_report_month"]["start_time"] = start_date.strftime("%Y-%m-%d 00:00:00")
            config["monthly_report_month"]["end_time"] = (end_date + timedelta(days=1)).strftime("%Y-%m-%d 02:00:00")

            with open(config_file_path, 'w') as file:
                json.dump(config, file, indent=4)

            messagebox.showinfo("æˆåŠŸ", f"æ—¥å ±å€é–“ç‚º {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}.\nè«‹ç¨ç­‰")

            entry_window.destroy()

            monthly_report_export(root,month_input)

        except ValueError:
            messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥ YYYYMM æ ¼å¼.")

    tk.Label(entry_window, text='è¼¸å…¥æœˆä»½ (YYYYMM):', font=('Microsoft JhengHei', 12, 'bold'), bg='lightblue').pack()
    entry = tk.Entry(entry_window, width=15, font=('Microsoft JhengHei', 18))
    entry.pack(padx=20, pady=5)

    save_button_style = {
        "font": ("Microsoft JhengHei", 12, "bold"),
        "bg": "Pink",           # ç¶ è‰²èƒŒæ™¯
        "fg": "Black",             # ç™½è‰²å­—é«”
        "activebackground": "Yellow",  # é»æ“Šæ™‚çš„èƒŒæ™¯è‰²
        "width": 20,
        "height": 2,
        "relief": "raised",        # æµ®å‡¸æ•ˆæœ
        "bd": 2                    # é‚Šæ¡†å¯¬åº¦
    }

    tk.Button(entry_window, text='ä¿å­˜å€é–“ï¼Œä¸¦é–‹å§‹ç”¢ç”Ÿ CSV',
            command=get_date_range, **save_button_style).pack(pady=5)

# æœˆå ±è£½ä½œ
def monthly_report_export(root,month_input):

    # æ—¥èªŒè·¯å¾‘
    temp_directory = "temp"
    log_file_path = "temp/monthly_logfile.log"
    error_log_path = "temp/monthly_error_log.log"

    # è¨­ç½®æ—¥èªŒ
    error_logger = setup_logging(temp_directory, log_file_path, error_log_path)

    try:
        # åŠ è¼‰é…ç½®
        config = load_config('.\config.json')

        if config:
            # æå– ClickUp ç›¸é—œé…ç½®
            team_id = config["clickup"]["team_id"]
            user_ids = [user["id"] for user in config["clickup"]["user_ids"]]
            user_name = [user["name"] for user in config["clickup"]["user_ids"]]
            
            start_unix_timestamp = convert_taiwan_time_str_to_unix_timestamp(config["monthly_report_month"]["start_time"])
            end_unix_timestamp = convert_taiwan_time_str_to_unix_timestamp(config["monthly_report_month"]["end_time"])

            # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrameï¼Œå­˜æ”¾clickup apiè³‡æ–™
            all_time_entries_df = pd.DataFrame()
            for user_id in user_ids:
                user_time_entries = get_time_entries(team_id, user_id, decrypted_config, start_unix_timestamp, end_unix_timestamp)

                if user_time_entries:
                    user_time_entries_df = pd.DataFrame(user_time_entries)
                    all_time_entries_df = pd.concat([all_time_entries_df, user_time_entries_df], ignore_index=True)

            original_all_time_entries_df = all_time_entries_df.copy()
            for col in original_all_time_entries_df.columns:
                all_time_entries_df = expand_dict_col(all_time_entries_df, col)

            logging.info("âœ… å·²æˆåŠŸè™•ç† ClickUp è³‡æ–™")

            # æå– Google Sheets ç›¸é—œé…ç½®
            credentials_info = config['googlesheet']
            spreadsheet_url = config['spreadsheet_url']['url']
            worksheet_index = config['spreadsheet_url'].get('worksheet_index', 0)
            # å¾ Google Sheets ç²å–è³‡æ–™ä½µè½‰æ›æˆ DataFrame
            project_info = fetch_data_from_google_sheets(credentials_info, spreadsheet_url, worksheet_index)
            logging.info("âœ… å·²æˆåŠŸè™•ç† Google Sheets è³‡æ–™")

            # å°è³‡æ–™æ¬„ä½å’Œå‹æ…‹é€²è¡Œè½‰æ›
            project_info.rename(columns={'List ID (KEY)': 'CU_LIST_ID'}, inplace=True)
            all_time_entries_df.rename(columns={'task_location_list_id': 'List_Id'}, inplace=True)
            project_info['CU_LIST_ID'] = pd.to_numeric(project_info['CU_LIST_ID'], errors='coerce')
            all_time_entries_df['List_Id'] = pd.to_numeric(all_time_entries_df['List_Id'], errors='coerce')
            logging.info("âœ… å·²æˆåŠŸè™•ç† DataFrame åˆ—åé‡å‘½åå’Œè³‡æ–™å‹æ…‹è½‰æ›")

            # ä½¿ç”¨å‡½ç¤ºå°è³‡æ–™é€²è¡Œåˆä½µ
            js_pjc = process_data(all_time_entries_df, project_info)
            logging.info("âœ… å·²æˆåŠŸè™•ç† DataFrame åˆä½µ")

            # ä½¿ç”¨å‡½ç¤ºæ–°å¢æ¬„ä½
            js_pjc['flattened_tags'] = js_pjc.apply(flatten_tags, axis=1)
            js_pjc['duration_overtime'] = js_pjc.apply(lambda x: x['duration'] if contains_overtime(x['tags']) else 0, axis=1)
            js_pjc.loc[js_pjc['tags'].apply(contains_overtime), 'duration'] = 0
            js_pjc['extracted_numbers'] = js_pjc['tags'].apply(extract_numbers)
            js_pjc['extracted_numbers'] = js_pjc['extracted_numbers'].str[0]
            logging.info("âœ… å·²æˆåŠŸè™•ç† DataFrame æ–°å¢æ¬„ä½")

            # é¸æ“‡éœ€è¦çš„åˆ—
            columns_to_select = ['user_id', 'user_username', 'start', 'ç³»çµ±ä»£ç¢¼', '*å°ˆæ¡ˆä»£ç¢¼','å·¥ä½œä»£ç¢¼', 'extracted_numbers', 'duration', 'duration_overtime','task_name', 'CU Folder']
            js_pjc_select = js_pjc[columns_to_select].copy()
            logging.info("âœ… å·²æˆåŠŸé¸æ“‡ç‰¹å®šåˆ—ä¸¦å‰µå»ºæ–°çš„ DataFrame")

            # é‡æ–°å‘½åæ¬„ä½
            js_pjc_select.rename(columns={'extracted_numbers': 'å·¥ä½œåˆ¥'}, inplace=True)
            js_pjc_select.rename(columns={'duration': 'æ­£å¸¸å·¥æ™‚'}, inplace=True)
            js_pjc_select.rename(columns={'duration_overtime': 'åŠ ç­å·¥æ™‚'}, inplace=True)
            js_pjc_select.rename(columns={'task_name': 'ä»»å‹™'}, inplace=True)
            js_pjc_select.rename(columns={'CU Folder': 'CUå°ˆæ¡ˆ'}, inplace=True)
            logging.info("âœ… å·²æˆåŠŸé‡æ–°å‘½åæ¬„ä½")

            # ä½¿ç”¨å‡½å¼è½‰æ›æ¬„ä½æˆå¹´æœˆæ—¥æ ¼å¼
            timestamp_columns = ['start']
            js_pjc_select = convert_multiple_timestamps(js_pjc_select, timestamp_columns)
            logging.info("âœ… å·²æˆåŠŸè½‰æ›æ¬„ä½æˆå¹´æœˆæ—¥æ ¼å¼")

            # ä½¿ç”¨å‡½å¼è½‰æ›æ¬„ä½æˆå°æ™‚å–®ä½
            columns_to_convert = ['åŠ ç­å·¥æ™‚', 'æ­£å¸¸å·¥æ™‚']
            js_pjc_select = convert_ms_to_hours(js_pjc_select, columns_to_convert)
            js_pjc_select['ç¸½å·¥æ™‚'] = js_pjc_select['åŠ ç­å·¥æ™‚'] + js_pjc_select['æ­£å¸¸å·¥æ™‚']
            logging.info("âœ… å·²æˆåŠŸè½‰æ›æ¬„ä½æˆå°æ™‚å–®ä½")

    except Exception as e:
        logging.error(f"âŒ è³‡æ–™è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    try:
        output_path = f"./OUTPUT/åˆè¦çµ„_{month_input}_æœˆå ±è¡¨_FCS.xlsx"
        os.makedirs("./OUTPUT", exist_ok=True)

        # é–‹å•Ÿçµ±ä¸€ ExcelWriter
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            workbook = writer.book

            grouped_by_user = js_pjc_select.groupby('user_username')

            for username, user_group in grouped_by_user:
                try:
                    # æ“·å–æª”å
                    first_name = username.split('.')[0] if '.' in username else username.split(' ')[0]

                    # å»º summary_rowsï¼ˆå¦‚å‰é¢é‚è¼¯ï¼‰
                    summary_rows = []
                    grouped_by_project = user_group.groupby('CUå°ˆæ¡ˆ')
                    for project, proj_df in grouped_by_project:
                        project_total = proj_df['ç¸½å·¥æ™‚'].sum()
                        summary_rows.append([project, round(project_total, 2)])

                        grouped_by_task = proj_df.groupby('ä»»å‹™')
                        for task, task_df in grouped_by_task:
                            task_total = task_df['ç¸½å·¥æ™‚'].sum()
                            summary_rows.append(['ã€€' + str(task), round(task_total, 2)])

                    # æ’å…¥æ¨™é¡Œã€æ¬„å
                    header_rows = [[f"{month_input} å·¥ä½œæœˆå ±è¡¨", ""], ["å°ˆæ¡ˆåç¨±/å·¥ä½œå…§å®¹", "æ™‚æ•¸"]]
                    summary_df = pd.DataFrame(header_rows + summary_rows)

                    # âœ… åŠ å…¥ç¸½è¨ˆåˆ—
                    project_rows = [row for row in summary_rows if isinstance(row[0], str) and not row[0].startswith('ã€€')]
                    total_hours = round(sum(row[1] for row in project_rows), 2)
                    summary_df.loc[len(summary_df.index)] = ['ç¸½è¨ˆ', total_hours]

                    # å»ºç«‹å·¥ä½œè¡¨
                    summary_df.to_excel(writer, index=False, header=False, sheet_name=first_name)
                    worksheet = writer.sheets[first_name]

                    # è¨­å®šæ¬„å¯¬
                    worksheet.set_column('A:A', 70)
                    worksheet.set_column('B:B', 7)

                    # å¯«å…¥æ¯æ ¼èˆ‡æ ¼å¼ï¼ˆèˆ‡å‰é¢æä¾›ä¸€è‡´ï¼‰
                    for row_idx in range(len(summary_df)):
                        val_col0 = summary_df.iloc[row_idx, 0]
                        val_col1 = summary_df.iloc[row_idx, 1]

                        is_header = row_idx < 2
                        is_project_row = isinstance(val_col0, str) and not val_col0.startswith('ã€€')
                        is_total_row = row_idx == len(summary_df) - 1

                        if is_header:
                            fmt_col0 = workbook.add_format({
                                'font_name': 'DFKai-SB', 'bold': True, 'align': 'center',
                                'valign': 'vcenter', 'border': 1
                            })
                            fmt_col1 = fmt_col0
                        elif is_project_row or is_total_row:
                            fmt_col0 = workbook.add_format({
                                'font_name': 'DFKai-SB', 'bold': True,
                                'border': 1, 'bg_color': '#D9E1F2'
                            })
                            fmt_col1 = workbook.add_format({
                                'font_name': 'DFKai-SB', 'bold': True,
                                'border': 1, 'bg_color': '#D9E1F2' #, 'num_format': '0.0'
                            })
                        else:
                            fmt_col0 = workbook.add_format({
                                'font_name': 'DFKai-SB', 'border': 1
                            })
                            fmt_col1 = workbook.add_format({
                                'font_name': 'DFKai-SB', 'border': 1 #, 'num_format': '0.0'
                            })

                        worksheet.write(row_idx, 0, val_col0, fmt_col0)
                        worksheet.write(row_idx, 1, val_col1, fmt_col1)

                    logging.info(f"âœ… åŠ å…¥ {username} å·¥ä½œè¡¨")

                except Exception as e:
                    logging.error(f"âŒ è™•ç†ä½¿ç”¨è€… {username} æ™‚éŒ¯èª¤ï¼š{e}")

        logging.info(f"ğŸ“ æ‰€æœ‰ä½¿ç”¨è€…å·¥ä½œæœˆå ±è¡¨å·²æˆåŠŸåŒ¯å‡ºã€‚")

    except Exception as e:
        logging.error(f"âŒ æª”æ¡ˆåŒ¯å‡ºæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        logging.info("âœ… æ‰€æœ‰å‹•ä½œå·²å®Œæˆï¼Œè«‹è‡³OUTPUTæŸ¥çœ‹æª”æ¡ˆ")
    finally:
        try:
            root.destroy()  # é—œé–‰ä¸»è¦–çª—
            messagebox.showinfo("å®Œæˆ", "å·²å®ŒæˆåŒ¯å‡º")  # å½ˆå‡ºæç¤ºè¨Šæ¯
        except Exception as e:
            logging.warning(f"âŒ ç„¡æ³•é—œé–‰è¦–çª—æˆ–é¡¯ç¤ºå®Œæˆè¨Šæ¯ï¼š{e}")

    root.destroy()  # é—œé–‰ä¸»è¦–çª—
    messagebox.showinfo("å®Œæˆ", "å·²å®ŒæˆåŒ¯å‡º") 

# é–‹å•ŸLog
def open_log_window(root, log_file_path):
    log_window = tk.Toplevel(root)
    log_window.title("Log Window")

    log_text = tk.Text(log_window, width=300, height=160)
    log_text.pack()

    try:
        # Specify the encoding here
        with open(log_file_path, 'r', encoding='utf-8') as file:
            log_text.insert(tk.END, file.read())
    except FileNotFoundError:
        log_text.insert(tk.END, "Log file not found.")
    except UnicodeDecodeError:
        log_text.insert(tk.END, "Error reading the log file: encoding issue.")

"""é‡‘é‘°åŠ å¯†è™•ç†"""
# ç”Ÿæˆçš„å¯†é‘°
key = b'h8ismKmlffQ56ReTOBLFTPkU8PlVfjyff2EwgQKrHTE='
# Fernet å¯†é‘°ç®—æ³•
cipher_suite = Fernet(key)
# å·²åŠ å¯†æ•¸æ“š
encrypted_data = b'gAAAAABlpfX8WV1yG2gyTkW1AGNeNCpBd4iCeG6hjlMPVmBnJbnmVc5cZ0OUYcAEmgFii7AX_pyH8zvQ4UsZXz8bzzEJsQ421LDkiAcmmZ9Mg5FauwUvGWYMZM_DkOC1hJNMuOtYjru0'
# è§£å¯†æ•¸æ“š
decrypted_config = cipher_suite.decrypt(encrypted_data)

"""é…ç½®æ–‡ä»¶æ™‚é–“å¹´æœˆæ—¥è½‰Unix"""
def convert_taiwan_time_str_to_unix_timestamp(time_str, time_format="%Y-%m-%d %H:%M:%S"):
    """å°‡å°ç£æ™‚é–“å­—ä¸²è½‰æ› Unix æ™‚é–“æˆ³ï¼ˆæ¯«ç§’ï¼‰"""
    #è§£ææ™‚é–“
    taiwan_timezone = pytz.timezone("Asia/Taipei")
    local_time = datetime.strptime(time_str, time_format)
    local_time = taiwan_timezone.localize(local_time)
    utc_time = local_time.astimezone(pytz.utc)
    # è½‰æ›ç‚º Unix
    unix_timestamp = int(utc_time.timestamp() * 1000)
    return unix_timestamp

"""clickup api è³‡æ–™çˆ¬å–"""
def get_time_entries(team_id, user_id, decrypted_config, start_unix_timestamp, end_unix_timestamp):
    """ç™¼èµ·è«‹æ±‚ç²å–æŒ‡å®šç”¨æˆ¶è³‡æ–™"""
    try:
        url = f"https://api.clickup.com/api/v2/team/{team_id}/time_entries"
        headers = {
            "Content-Type": "application/json",
            "Authorization": decrypted_config
        }
        query_params = {
            "assignee": str(user_id),
            "start_date": start_unix_timestamp,
            "end_date": end_unix_timestamp
        }
        response = requests.get(url, headers=headers, params=query_params)

        if response.status_code == 200:
            logging.info(f"âœ… æˆåŠŸå–å¾—ç”¨æˆ¶ {user_id} ")
            return response.json().get('data', [])
        else:
            logging.warning(f"âŒ ç”¨æˆ¶ {user_id} çš„è³‡æ–™ç²å–å¤±æ•—: {response.text}")
            return []

    except Exception as e:
        logging.error(f"âŒ åœ¨å–å¾—ç”¨æˆ¶ {user_id} çš„è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

"""google sheet api è³‡æ–™çˆ¬å–"""
def fetch_data_from_google_sheets(credentials_info, spreadsheet_url, worksheet_index=0):
    # è¨­ç½® Google Sheets API èªè­‰
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_info, scope)
    gc = gspread.authorize(credentials)

    # æ‰“é–‹æŒ‡å®šçš„ Google Sheets å·¥ä½œè¡¨
    workbook = gc.open_by_url(spreadsheet_url)
    worksheet = workbook.get_worksheet(worksheet_index)

    # å°‡è³‡æ–™è½‰æˆDataFrame
    data = worksheet.get_all_values()
    df = pd.DataFrame(data)
    df.columns = df.iloc[0]
    df = df[1:]
    return df

"""è‡ªå®šç¾©å‡½æ•¸"""

"""å®šç¾©åˆä½µgooglesheetå’Œclickupå‡½å¼"""
def process_data(all_entries, df):
    if 'List_Id' in all_entries.columns and 'CU_LIST_ID' in df.columns:
        processed_data = pd.merge(all_entries, df, left_on='List_Id', right_on='CU_LIST_ID', how='left')
        logging.info("âœ… clickup_timesheet å’Œ project_info è³‡æ–™åˆä½µæˆåŠŸ")
    else:
        logging.error("âŒ åˆä½µå¤±æ•—ï¼šç¼ºå°‘å¿…è¦çš„åˆ—")
        processed_data = pd.DataFrame()
    return processed_data

"""å®šç¾©å±•é–‹å­—å…¸é¡å‹çš„åˆ—å‡½å¼"""
def expand_dict_col(df, column):
    if isinstance(df[column].iloc[0], dict):
        expanded_cols = df[column].apply(pd.Series)
        expanded_cols = expanded_cols.rename(lambda x: f"{column}_{x}", axis='columns')
        df = pd.concat([df.drop(column, axis=1), expanded_cols], axis=1)
    return df

"""å®šç¾©æ‹†è§£tagæ¬„ä½å‡½å¼"""
def flatten_tags(row):
    tags = row['tags']
    if isinstance(tags, str):
        tags = json.loads(tags.replace("'", '"'))
    return [tag['name'] for tag in tags if 'name' in tag]

"""å®šç¾©æª¢æŸ¥'overtime'å‡½å¼"""
def contains_overtime(tag_list):
    return any(tag.get('name') == 'overtime' for tag in tag_list)

"""å®šç¾©å°tagså­—å…¸æ¬„ä½æ¢ä»¶å–å€¼å‡½å¼"""
def extract_numbers(tags):
    numbers = []
    try:
        if tags and isinstance(tags, list):
            for item in tags:
                if isinstance(item, dict) and 'name' in item:
                    name = item['name']
                    number = name.split('_')[0] if '_' in name else None
                    if number:
                        numbers.append(number)
        logging.debug(f"å¾nameæå–æ•¸å­—: {numbers}")
        return numbers
    except Exception as e:
        logging.error(f"âŒ åœ¨æå–æ•¸å­—éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

"""å®šç¾©æ—¥æœŸè½‰æ›æˆå¹´æœˆæ—¥å‘ˆç¾å‡½å¼"""
def convert_multiple_timestamps(df, timestamp_columns, timezone='Asia/Taipei'):
    try:
        tz = pytz.timezone(timezone)
        for column in timestamp_columns:
            # è½¬æ¢æ¯ä¸ªæŒ‡å®šçš„æ—¶é—´æˆ³åˆ—
            df[column] = pd.to_datetime(df[column], unit='ms', utc=True)
            df[column] = df[column].dt.tz_convert(tz).dt.date
        return df
    except Exception as e:
        print(f"âŒ è½‰æ›æ™‚é–“æˆ³è¨˜ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

"""å®šç¾©æ¯«ç§’è½‰æ›æˆå°æ™‚å‡½å¼"""
def convert_ms_to_hours(df, columns):
    ms_to_hours = 3600000
    for column in columns:
        try:
            df[column] = pd.to_numeric(df[column], errors='coerce')
            df[column] = (df[column] / ms_to_hours).apply(lambda x: np.floor(x * 10) / 10)
        except KeyError:
            print(f"'{column}'ä¸å­˜åœ¨åœ¨è³‡æ–™é›†")
        except Exception as e:
            print(f"è½‰æ›å°æ™‚å–®ä½æ™‚ç™¼ç”ŸéŒ¯èª¤ '{column}': {e}")
    return df

"""å®šç¾©è«‹å‡ç™»æ‰“æª¢æŸ¥å‡½å¼"""
def check_leave_work_type(df, work_code, work_type, error_logger):
    try:
        for start, group_df in df.groupby('start'):
            filtered_df = group_df[(group_df['å·¥ä½œä»£ç¢¼'] == work_code) & (group_df['å·¥ä½œåˆ¥'] != work_type)]
            is_error_found = False
            error_message = ""

            for index, row in filtered_df.iterrows():
                is_error_found = True
                error_logger.error(f"ğŸ”´ {row['start']}, {row['user_username']} çš„è«‹å‡å·¥ä½œåˆ¥æœ‰å•é¡Œ\n")

            if not is_error_found:
                error_logger.info(f"ğŸŸ¢ {start} æ²’æœ‰ç™¼ç¾ä»»ä½•å·¥ä½œä»£ç¢¼ç‚º '{work_code}' ä¸”å·¥ä½œåˆ¥ä¸ç­‰æ–¼ {work_type} çš„æƒ…æ³")

    except Exception as e:
        error_logger.error(f"âŒ åˆ†çµ„å’Œç¯©é¸éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

"""å®šç¾©è«‹å‡å·¥ä½œåˆ¥æª¢æŸ¥å‡½å¼"""
def check_leave_work_type(df, work_code, work_type, error_logger):
    try:
        # ç¢ºä¿ DataFrame ä¸­çš„ 'å·¥ä½œä»£ç¢¼' å’Œ 'å·¥ä½œåˆ¥' æ¬„ä½æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        # å¦‚æœåŸæœ¬ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå‰‡é€²è¡Œè½‰æ›
        df['å·¥ä½œä»£ç¢¼'] = df['å·¥ä½œä»£ç¢¼'].astype(str)
        df['å·¥ä½œåˆ¥'] = df['å·¥ä½œåˆ¥'].astype(str)

        # åŒæ¨£ç¢ºä¿å‚³å…¥å‡½å¼çš„ work_code å’Œ work_type åƒæ•¸æ˜¯å­—ç¬¦ä¸²
        work_code = str(work_code)
        work_type = str(work_type)

        # éæ­·æŒ‰ 'start' åˆ†çµ„çš„ DataFrame
        for start, group_df in df.groupby('start'):
            # ç¯©é¸å‡ºç¬¦åˆç‰¹å®šå·¥ä½œä»£ç¢¼ä¸”å·¥ä½œåˆ¥ä¸ç¬¦åˆæ¢ä»¶çš„æ•¸æ“š
            filtered_df = group_df[(group_df['å·¥ä½œä»£ç¢¼'] == work_code) & (group_df['å·¥ä½œåˆ¥'] != work_type)]
            is_error_found = False
            error_message = ""

            # æª¢æŸ¥ç¯©é¸å‡ºçš„æ•¸æ“šï¼Œä¸¦æ§‹å»ºéŒ¯èª¤ä¿¡æ¯
            for index, row in filtered_df.iterrows():
                is_error_found = True
                error_logger.error(f"{'ğŸ”´'} {row['start']}, {row['user_username']} çš„è«‹å‡å·¥ä½œåˆ¥æœ‰å•é¡Œ\n")

            # æ ¹æ“šæ˜¯å¦ç™¼ç¾éŒ¯èª¤è¨˜éŒ„ä¸åŒçš„æ—¥èªŒä¿¡æ¯
            if not is_error_found:
                error_logger.info(f"ğŸŸ¢ {start} æ²’æœ‰ç™¼ç¾ä»»ä½•å·¥ä½œä»£ç¢¼ç‚º '{work_code}' ä¸”å·¥ä½œåˆ¥ä¸ç­‰æ–¼ {work_type} çš„æƒ…æ³")

    except Exception as e:
        # è™•ç†ä¸¦è¨˜éŒ„ä»»ä½•åœ¨éç¨‹ä¸­å‡ºç¾çš„ç•°å¸¸
        error_logger.error(f"åˆ†çµ„å’Œç¯©é¸éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

"""å®šç¾©å·¥ä½œåˆ¥æª¢æŸ¥å‡½å¼"""
def check_work_category(df, valid_categories, error_logger):
    try:
        for start, group_df in df.groupby('start'):
            filtered_category = group_df[~group_df['å·¥ä½œåˆ¥'].isin(valid_categories)]

            is_error_found = False
            error_message = ""

            for index, row in filtered_category.iterrows():
                is_error_found = True
                username = row['user_username']
                error_logger.error(f"{'ğŸ”´'} {start}, {username} å­˜åœ¨å·¥ä½œåˆ¥ä¸æ­£ç¢ºæƒ…æ³\n")

            if not is_error_found:
                error_logger.info(f"ğŸŸ¢ {start} æ²’æœ‰å·¥ä½œåˆ¥ä¸æ­£ç¢ºæƒ…æ³")

    except Exception as e:
        error_logger.error(f"{'âŒ '} åˆ†çµ„å’Œç¯©é¸ 'å·¥ä½œåˆ¥' éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

"""å®šç¾©æ­£å¸¸å·¥æ™‚è¶…æ™‚æª¢æŸ¥å‡½å¼"""
def check_normal_working_hours(df, max_hours, error_logger):

    config = load_config('.\config.json')
    try:
        grouped_by_date = df.groupby('start')
        for start, group in grouped_by_date:
            grouped = group.groupby(['user_username', 'user_id'])['æ­£å¸¸å·¥æ™‚'].sum()

            is_error_found = False
            error_message = ""

            user_list = config["clickup"]["user_ids"]
            grouped_user_ids = {uid for (_, uid) in grouped.keys()}

            for user in user_list:
                user_id = user["id"]
                user_name = user["name"]
                
                if user_id not in grouped_user_ids:
                    is_error_found = True
                    error_logger.error(f"{'ğŸ”´'} {start}, {user_name} æœªå¡«å·¥æ™‚ã€‚")

            for (user, user_id), total_hours in grouped.items():
                if total_hours > max_hours:
                    is_error_found = True
                    error_logger.error(f"{'ğŸ”´'} {start}, {user} çš„æ­£å¸¸å·¥æ™‚è¶…é {total_hours} å°æ™‚ã€‚")
                elif total_hours < max_hours:
                    is_error_found = True
                    error_logger.error(f"{'ğŸ”´'} {start}, {user} çš„æ­£å¸¸å·¥æ™‚æœªæ»¿ {max_hours} å°æ™‚ã€‚")

            if not is_error_found:
                error_logger.info(f"ğŸŸ¢ {start} æ²¡æœ‰æ­£å¸¸å·¥æ™‚æ‰“éŒ¯æƒ…å†µ")

    except Exception as e:
        error_logger.error(f"åˆ†çµ„å’Œç¯©é¸ 'æ­£å¸¸å·¥æ™‚' éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

"""å®šç¾©å·¥æ™‚å–®ä½ç™»æ‰“æª¢æŸ¥å‡½å¼"""
def check_time_unit_compliance(df, error_logger):
    try:
        from decimal import ROUND_HALF_UP, Decimal
        df['æ­£å¸¸å·¥æ™‚'] = df['æ­£å¸¸å·¥æ™‚'].apply(lambda x: Decimal(x).quantize(Decimal('1'), rounding=ROUND_HALF_UP))
        df['åŠ ç­å·¥æ™‚'] = df['åŠ ç­å·¥æ™‚'].apply(lambda x: Decimal(x).quantize(Decimal('1'), rounding=ROUND_HALF_UP))
        condition_normal = (df['æ­£å¸¸å·¥æ™‚'] * 60) - 6 * ((df['æ­£å¸¸å·¥æ™‚'] * 60) // 6) != 0
        condition_overtime = (df['åŠ ç­å·¥æ™‚'] * 60) - 6 * ((df['åŠ ç­å·¥æ™‚'] * 60) // 6) != 0
        condition = condition_normal | condition_overtime

        for date, group_divide in df.groupby('start'):
            found_non_divisible = False
            error_message = ""

            for index, row in group_divide.iterrows():
                if condition_normal[index] or condition_overtime[index]:
                    found_non_divisible = True
                    error_logger.debug(f"{'ğŸ”´'} {date}, {row['user_username']} çš„å·¥æ™‚å–®ä½éŒ¯èª¤\n")

            if found_non_divisible:
                logging.warning(f"{'ğŸ”´'} {date} çš„ä»¥ä¸‹ç´€éŒ„å·¥æ™‚å–®ä½éŒ¯èª¤:\n{error_message}")
                
            else:
                error_logger.info(f"ğŸŸ¢ {date} æ²¡æœ‰æ™‚é–“å–®ä½ä¸ç¬¦åˆæƒ…å†µ")

    except Exception as e:
        error_logger.error(f"ğŸ”´ æª¢æŸ¥å·¥æ™‚è¢«6æ•´é™¤ç™¼ç”ŸéŒ¯èª¤: {e}")

def export_data(root):

    # æ—¥èªŒè·¯å¾‘
    temp_directory = "temp"
    log_file_path = "temp/logfile.log"
    error_log_path = "temp/error_log.log"

    # è¨­ç½®æ—¥èªŒ
    error_logger = setup_logging(temp_directory, log_file_path, error_log_path)

    try:
        # åŠ è¼‰é…ç½®
        config = load_config('.\config.json')

        if config:
            # æå– ClickUp ç›¸é—œé…ç½®
            team_id = config["clickup"]["team_id"]
            user_ids = [user["id"] for user in config["clickup"]["user_ids"]]
            user_name = [user["name"] for user in config["clickup"]["user_ids"]]
            
            start_unix_timestamp = convert_taiwan_time_str_to_unix_timestamp(config["time_range"]["start_time"])
            end_unix_timestamp = convert_taiwan_time_str_to_unix_timestamp(config["time_range"]["end_time"])

            # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrameï¼Œå­˜æ”¾clickup apiè³‡æ–™
            all_time_entries_df = pd.DataFrame()
            for user_id in user_ids:
                user_time_entries = get_time_entries(team_id, user_id, decrypted_config, start_unix_timestamp, end_unix_timestamp)

                if user_time_entries:
                    user_time_entries_df = pd.DataFrame(user_time_entries)
                    all_time_entries_df = pd.concat([all_time_entries_df, user_time_entries_df], ignore_index=True)

            original_all_time_entries_df = all_time_entries_df.copy()
            for col in original_all_time_entries_df.columns:
                all_time_entries_df = expand_dict_col(all_time_entries_df, col)

            logging.info("âœ… å·²æˆåŠŸè™•ç† ClickUp è³‡æ–™")

            # æå– Google Sheets ç›¸é—œé…ç½®
            credentials_info = config['googlesheet']
            spreadsheet_url = config['spreadsheet_url']['url']
            worksheet_index = config['spreadsheet_url'].get('worksheet_index', 0)
            # å¾ Google Sheets ç²å–è³‡æ–™ä½µè½‰æ›æˆ DataFrame
            project_info = fetch_data_from_google_sheets(credentials_info, spreadsheet_url, worksheet_index)
            logging.info("âœ… å·²æˆåŠŸè™•ç† Google Sheets è³‡æ–™")

            # å°è³‡æ–™æ¬„ä½å’Œå‹æ…‹é€²è¡Œè½‰æ›
            project_info.rename(columns={'List ID (KEY)': 'CU_LIST_ID'}, inplace=True)
            all_time_entries_df.rename(columns={'task_location_list_id': 'List_Id'}, inplace=True)
            project_info['CU_LIST_ID'] = pd.to_numeric(project_info['CU_LIST_ID'], errors='coerce')
            all_time_entries_df['List_Id'] = pd.to_numeric(all_time_entries_df['List_Id'], errors='coerce')
            logging.info("âœ… å·²æˆåŠŸè™•ç† DataFrame åˆ—åé‡å‘½åå’Œè³‡æ–™å‹æ…‹è½‰æ›")

            # ä½¿ç”¨å‡½ç¤ºå°è³‡æ–™é€²è¡Œåˆä½µ
            js_pjc = process_data(all_time_entries_df, project_info)
            logging.info("âœ… å·²æˆåŠŸè™•ç† DataFrame åˆä½µ")

            # ä½¿ç”¨å‡½ç¤ºæ–°å¢æ¬„ä½
            js_pjc['flattened_tags'] = js_pjc.apply(flatten_tags, axis=1)
            js_pjc['duration_overtime'] = js_pjc.apply(lambda x: x['duration'] if contains_overtime(x['tags']) else 0, axis=1)
            js_pjc.loc[js_pjc['tags'].apply(contains_overtime), 'duration'] = 0
            js_pjc['extracted_numbers'] = js_pjc['tags'].apply(extract_numbers)
            js_pjc['extracted_numbers'] = js_pjc['extracted_numbers'].str[0]
            logging.info("âœ… å·²æˆåŠŸè™•ç† DataFrame æ–°å¢æ¬„ä½")

            # é¸æ“‡éœ€è¦çš„åˆ—
            # columns_to_select = ['user_id', 'user_username', 'start', 'ç³»çµ±ä»£ç¢¼', '*å°ˆæ¡ˆä»£ç¢¼','å°ˆæ¡ˆåç¨±(PJC)/ç³»çµ±åç¨±','å·¥ä½œä»£ç¢¼', 'extracted_numbers', 'duration', 'duration_overtime','task_name']
            columns_to_select = ['user_id', 'user_username', 'start', 'ç³»çµ±ä»£ç¢¼', '*å°ˆæ¡ˆä»£ç¢¼','å·¥ä½œä»£ç¢¼', 'extracted_numbers', 'duration', 'duration_overtime','task_name']
            js_pjc_select = js_pjc[columns_to_select].copy()
            logging.info("âœ… å·²æˆåŠŸé¸æ“‡ç‰¹å®šåˆ—ä¸¦å‰µå»ºæ–°çš„ DataFrame")

            # ä½¿ç”¨ .loc ä¾†æ›´æ–° 'task.name' åˆ—
            js_pjc_select.loc[:, 'task_name'] = '(' + js_pjc['task_custom_id'] + ') ' + js_pjc_select['task_name'] + ' : ' + js_pjc['description']
            logging.info("âœ… å·²æ›´æ–° 'task_name' åˆ—")

            # é‡æ–°å‘½åæ¬„ä½
            js_pjc_select.rename(columns={'extracted_numbers': 'å·¥ä½œåˆ¥'}, inplace=True)
            js_pjc_select.rename(columns={'duration': 'æ­£å¸¸å·¥æ™‚'}, inplace=True)
            js_pjc_select.rename(columns={'duration_overtime': 'åŠ ç­å·¥æ™‚'}, inplace=True)
            js_pjc_select.rename(columns={'task_name': 'å·¥ä½œèªªæ˜'}, inplace=True)
            logging.info("âœ… å·²æˆåŠŸé‡æ–°å‘½åæ¬„ä½")

            # ä½¿ç”¨å‡½å¼è½‰æ›æ¬„ä½æˆå¹´æœˆæ—¥æ ¼å¼
            timestamp_columns = ['start']
            js_pjc_select = convert_multiple_timestamps(js_pjc_select, timestamp_columns)
            logging.info("âœ… å·²æˆåŠŸè½‰æ›æ¬„ä½æˆå¹´æœˆæ—¥æ ¼å¼")

            # ä½¿ç”¨å‡½å¼è½‰æ›æ¬„ä½æˆå°æ™‚å–®ä½
            columns_to_convert = ['åŠ ç­å·¥æ™‚', 'æ­£å¸¸å·¥æ™‚']
            js_pjc_select = convert_ms_to_hours(js_pjc_select, columns_to_convert)
            logging.info("âœ… å·²æˆåŠŸè½‰æ›æ¬„ä½æˆå°æ™‚å–®ä½")

            # ä½¿ç”¨å‡½å¼æª¢æŸ¥è¦å‰‡ä½µå¯«å…¥error_log
            pjc_check = js_pjc_select.copy()
            check_leave_work_type(pjc_check, '00000000', 16, error_logger)
            valid_categories = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']
            check_work_category(pjc_check, valid_categories, error_logger)
            check_normal_working_hours(pjc_check, 8, error_logger)
            check_time_unit_compliance(pjc_check, error_logger)
            logging.info("âœ… å·²æˆåŠŸåŒ¯å‡ºerror_log")

    except Exception as e:
        logging.error(f"âŒ è³‡æ–™è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    try:
    #     # é¦–å…ˆæŒ‰æ—¥æœŸåˆ†ç»„
        js_pjc_select['start'] = pd.to_datetime(js_pjc_select['start'])
        grouped_by_date = js_pjc_select.groupby(js_pjc_select['start'].dt.date)

        # ç‚ºæ¯ä¸€å€‹æ—¥æœŸå»ºç«‹ä¸€å€‹æ–‡ä»¶å¤¾ä¸¦æŒ‡å®šåŒ¯å‡ºè³‡æ–™å¤¾
        for date, date_group in grouped_by_date:
            date_format = date.strftime("%Y%m%d")
            date_folder_path = f".\OUTPUT\{date_format}"
            if not os.path.exists(date_folder_path):
                os.makedirs(date_folder_path)
                print(f"æ—¥æœŸæ–‡ä»¶å¤¾å·²å»ºç«‹ï¼š{date_folder_path}")
            else:
                print(f"æ—¥æœŸæ–‡ä»¶å¤¾å·²å»ºç«‹ï¼š{date_folder_path}")

            # æŒ‰ç”¨æˆ¶åˆ†ç»„æˆæ¯ä¸€å€‹æª”æ¡ˆ
            grouped_by_user = date_group.groupby('user_username')
            for username, user_group in grouped_by_user:
                try:
                    # æ–‡ä»¶åè¨­å®š
                    if '.' in username:
                        first_name = username.split('.')[0]
                    else:
                        first_name = username.split(' ')[0]

                    # 'start'åˆ—æ˜¯datetimeæ ¼å¼ï¼Œä¸¦æ ¼å¼åŒ–ç‚º'YYYY-MM-DD'
                    user_group['start'] = user_group['start'].dt.strftime('%Y/%m/%d')

                    # åˆªé™¤ä¸å¿…è¦çš„åˆ—
                    columns_to_drop = ['user_id', 'user_username']
                    user_group.drop(columns=columns_to_drop, inplace=True, errors='ignore')

                    # å»ºç«‹æ–‡ä»¶åä¸¦ä¿å­˜DataFrameç‚ºCSVæ–‡ä»¶
                    filename = os.path.join(date_folder_path, f"PJC_{first_name}.csv")
                    user_group.to_csv(filename, encoding='big5', index=False, header=False)
                    logging.info(f"ğŸ“ å„²å­˜dataframe {username} åˆ° CSV æª”æ¡ˆ {filename}")

                except Exception as e:
                    logging.error(f"âŒ è™•ç†ç”¨æˆ¶å {username} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

                logging.info(f"âœ… æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸåŒ¯å‡ºè‡³ '{date_folder_path}'ã€‚")

    except Exception as e:
        logging.error(f"âŒ æª”æ¡ˆåŒ¯å‡ºæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        logging.info("âœ… æ‰€æœ‰å‹•ä½œå·²å®Œæˆï¼Œè«‹è‡³OUTPUTæŸ¥çœ‹æª”æ¡ˆ")
    finally:
        try:
            root.destroy()  # é—œé–‰ä¸»è¦–çª—
            messagebox.showinfo("å®Œæˆ", "å·²å®ŒæˆåŒ¯å‡º")  # å½ˆå‡ºæç¤ºè¨Šæ¯
        except Exception as e:
            logging.warning(f"âŒ ç„¡æ³•é—œé–‰è¦–çª—æˆ–é¡¯ç¤ºå®Œæˆè¨Šæ¯ï¼š{e}")

"""ä¸»ç¨‹å¼"""
def main():

    root = tk.Tk()
    root.title("PJCåŒ¯å‡ºå·¥å…·")
    root.geometry("300x200")
    root.configure(bg='lightblue')  # è¨­å®šåº•è‰²

    # æ—¥èªŒè·¯å¾‘
    log_file_path = "temp/logfile.log"

    # å»ºç«‹æ—¥æœŸé¸æ“‡å™¨å’Œæ—¥èªŒæŸ¥çœ‹æŒ‰éˆ•
    def create_hover_button(parent, text, command, normal_bg="Pink", hover_bg="HotPink"):
        # å»ºç«‹å…©ç¨®å­—å‹ï¼šæ­£å¸¸ / ç²—é«”
        normal_font = tkfont.Font(family="Microsoft JhengHei", size=12, weight="normal")
        bold_font = tkfont.Font(family="Microsoft JhengHei", size=12, weight="bold")

        # å»ºç«‹æŒ‰éˆ•
        button = tk.Button(
            parent,
            text=text,
            command=command,
            width=25,
            height=2,
            bg=normal_bg,
            fg="Black",
            font=normal_font,
            relief="raised",
            activebackground="#45a049",
        )
        button.pack(pady=5)

        # æ»‘å…¥æ»‘å‡ºäº‹ä»¶ï¼šæ”¹å­—é«”èˆ‡èƒŒæ™¯è‰²
        button.bind("<Enter>", lambda e: [button.config(bg=hover_bg, font=bold_font)])
        button.bind("<Leave>", lambda e: [button.config(bg=normal_bg, font=normal_font)])

        return button

    # å»ºç«‹æŒ‰éˆ•
    create_hover_button(root, "åŒ¯å‡ºPJC", lambda: create_date_selector(root, config_file_path))
    create_hover_button(root, "æœˆå ±ä¸‹è¼‰", lambda: monthly_report(root, config_file_path))
    create_hover_button(root, "æŸ¥çœ‹æ—¥èªŒ", lambda: open_log_window(root, log_file_path))

    root.mainloop()
    
if __name__ == "__main__":
    main()
